{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Model overfitting, validation, process models\n",
    "\n",
    "At this point, we have had our first experience with analysing a data set with a machine learning method (e.g. kNN)\n",
    "and evaluating the results.\n",
    "In this module, we will discuss the problem of model overfitting and how to validate the model.\n",
    "We will also introduce some the CRISP-DM process model, as well as other process models, which create much-needed structure to the data analysis process.\n",
    "\n",
    "## Model overfitting\n",
    "\n",
    "Whenever we apply a machine learning method to a data set, the goal is to construct a trained model that generalizes well to new, unseen data. For example, decision trees and trained neural networks are models that can be used to predict the class of a new data point. The kNN algorithm that we previously studied is conceptually slightly different: there the model is the training data itself, and the prediction is made by comparing the new data point to the training data.\n",
    "\n",
    "Irrespective of the method, the predictions rely on the training data, or, expressed differently, on a model that grasps the essence of the training data.\n",
    "\n",
    "\n",
    "The problem of model overfitting stems from the fact that a constructed model, as it grasps the characteristics of the training data, always adapts to the specific peculiarities of the training data. These peculiarities can be noise in the data, or they can be patterns that are not generalizable to new data.\n",
    "\n",
    "Consider an extreme example where we collect a group of people (say 30 persons), and try to predict who of them will be left-handed. As explanatory variables, we might use a large number of easily measurable quantities, such as height, weight, age, etc. We might also include some more exotic variables, such as the number of freckles on the person's face. If we have enough variables, we can construct a model that predicts the left-handedness of the person with 100% accuracy. The model might tell that, for example, if you are 1.75 meters tall, weigh 70 kg, and have 10 freckles on your face, you are left-handed. This model is overfitted, as it is based on the peculiarities of the training data, and not on generalizable patterns.\n",
    "\n",
    "Even though this example is extreme, the danger of model overfitting is real in all machine learning applications. The goal of the data analyst is to construct a model that generalizes well to new data, and does not overfit to the training data.\n",
    "\n",
    "In our extreme example, we can easily test whether the model generalizes well to new data by collecting a new group of people and testing the model on them. Almost inevitably, the model will fail to predict the left-handedness of the new group of people, as the model is based on the peculiarities of the training data. The accuracy of the classifier in this new data would probably be comparable to random guessing.\n",
    "\n",
    "The act of collecting new data to test the model is called validation. In the following sections, we will discuss different ways to validate a model.\n",
    "\n",
    "## Validation\n",
    "\n",
    "Validation means exposing the model to new data to test its generalization capabilities. The measures of goodness for a classifier, such as accuracy, precision, and recall should always be estimated from data that has not been used to train the model. That way, we can be sure that the model generalizes well to new data, or, in other words, is not subject to overfitting.\n",
    "\n",
    "There are various ways to validate a model. In the following sections, we will discuss some of the most common ones.\n",
    "\n",
    "### Split validation\n",
    "\n",
    "Split validation means that a part of the data is used to train the model, and another part is used to test the model. The simplest way to do this is to split the data into two parts: a training set and a test set. The training set is used to train the model, and the test set is used to evaluate the model. In most cases, the split is done randomly.\n",
    "\n",
    "It is a common practice to use roughty two-thirds of the data for training and one-third for testing. The exact ratio depends on the size of the data set and the problem at hand.\n",
    "\n",
    "Let us use the Iris data set to illustrate split validation, in conjunction with the kNN classifier. The following code snippet demonstrates how to split the data into training and test sets, and how to train and evaluate the kNN classifier.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of kNN classifier on the test set: 0.98\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = pd.read_csv('datasets/iris/iris.csv').drop(columns = ['sepal_length','sepal_width'])\n",
    "\n",
    "X = iris.drop(columns = 'species')\n",
    "y = iris['species']\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "# 70% of the data will be used for training, 30% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12345)\n",
    "\n",
    "# Create a kNN classifier\n",
    "# n_neighbors parameter specifies the number of neighbors to use (k)\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Train the kNN classifier on the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained classifier to predict labels for the test set\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Calculate and print the accuracy of the classifier on the test set\n",
    "accuracy_test = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of kNN classifier on the test set: {accuracy_test:.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T12:58:01.462413600Z",
     "start_time": "2024-08-15T12:58:01.414280Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For comparison, the following code block calculates the accuracy of the kNN classifier on the training set. In many cases, the accuracy on the training set is higher than the accuracy on the test set, as the model is trained on the training data, and therefore performs better on the training data than on new, unseen data. Do not rely on the training accuracy as a measure of the model's performance!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of kNN classifier on the training set: 0.99 (EXERCISE CAUTION!)\n"
     ]
    }
   ],
   "source": [
    "# This is just to show the difference between training and test accuracy\n",
    "y_train_pred = knn.predict(X_train)\n",
    "accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "print(f\"Accuracy of kNN classifier on the training set: {accuracy_train:.2f} (EXERCISE CAUTION!)\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T12:58:01.463462400Z",
     "start_time": "2024-08-15T12:58:01.437013400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> As you run the code, you may occasionally get different accuracy values, as the data is split randomly into training and test sets. As the Iris data set is relatively small and easy for the kNN classifier, the accuracy is expected to be high even for the test set, and randomness in the data split may cause the accuracy to vary.\n",
    ">\n",
    "> Generally, the model overfitting becomes worse as the number of features, and, consequently, model complexity increases. In the Iris data set, we have only four features, which makes the problem relatively simple. In more complex problems, the risk of overfitting is higher.\n",
    ">\n",
    "In the sklearn library, the `train_test_split` function is used to split the data into training and test sets. The `test_size` parameter specifies the proportion of the data that should be used for testing. The rows of the data frame are split randomly, so each run of the code may give slightly different results (unless the random seed is fixed). The optional `random_state` parameter can be used to fix the random seed, which ensures that the data is split in the same way each time the code is run. This can be useful for reproducibility. As the parameter value, just choose an integer, e.g. 12345. Never tune the value to get favorable results, as this would be a form of malpractice called cherry picking."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
